# WebCrawler Pro - Full Stack Web Crawling Platform

A comprehensive web crawling platform with a modern React frontend, Python backend API, and MongoDB job management.

## ğŸ—ï¸ Project Structure

```
donut-bot/
â”œâ”€â”€ backend/                 # Python backend API
â”‚   â”œâ”€â”€ donutbot/           # Core crawler modules
â”‚   â”œâ”€â”€ web_crawler.py      # Main crawler implementation
â”‚   â”œâ”€â”€ api_crawler.py      # API server entry point
â”‚   â”œâ”€â”€ api_client_example.py
â”‚   â”œâ”€â”€ requirements.txt    # Python dependencies
â”‚   â”œâ”€â”€ Dockerfile         # Backend container
â”‚   â”œâ”€â”€ logs/              # Application logs
â”‚   â””â”€â”€ output/            # Crawled data output
â”œâ”€â”€ frontend/               # React TypeScript frontend
â”‚   â”œâ”€â”€ src/               # React source code
â”‚   â”œâ”€â”€ public/            # Static assets
â”‚   â”œâ”€â”€ package.json       # Node.js dependencies
â”‚   â””â”€â”€ Dockerfile         # Frontend container
â”œâ”€â”€ config/                 # Configuration files
â”‚   â”œâ”€â”€ docker-compose.yml # Docker orchestration
â”‚   â”œâ”€â”€ mongo-init.js      # MongoDB initialization
â”‚   â””â”€â”€ nginx.conf         # Nginx configuration
â”œâ”€â”€ scripts/                # Utility scripts
â”‚   â”œâ”€â”€ start.sh           # Start all services
â”‚   â””â”€â”€ stop.sh            # Stop all services
â”œâ”€â”€ docs/                   # Documentation
â”‚   â”œâ”€â”€ README.md          # Detailed documentation
â”‚   â”œâ”€â”€ DEPLOYMENT.md      # Deployment guide
â”‚   â””â”€â”€ LICENSE            # Project license
â””â”€â”€ README.md              # This file
```

## ğŸš€ Quick Start

### Prerequisites
- Docker and Docker Compose
- Node.js 18+ (for local frontend development)
- Python 3.11+ (for local backend development)

### Using Docker (Recommended)

1. **Start all services:**
   ```bash
   cd config
   docker-compose up --build
   ```

2. **Access the applications:**
   - Frontend: http://localhost:3000
   - Backend API: http://localhost:8089
   - Kafka UI: http://localhost:8080 (optional monitoring)
   - Redis Commander: http://localhost:8081 (optional monitoring)

### Local Development

1. **Backend:**
   ```bash
   cd backend
   pip install -r requirements.txt
   python api_crawler.py
   ```

2. **Frontend:**
   ```bash
   cd frontend
   npm install
   npm start
   ```

## ğŸ› ï¸ Features

### Backend API
- **RESTful API** for crawler control
- **MongoDB integration** for job management
- **Redis caching** and queue management
- **Kafka streaming** for data processing
- **Health monitoring** and metrics

### Frontend Dashboard
- **Real-time monitoring** of crawler status
- **Job management** with create, start, pause, stop
- **Performance metrics** and analytics
- **Modern UI** with Tailwind CSS
- **TypeScript** for type safety

### Infrastructure
- **Docker containers** for easy deployment
- **MongoDB** for persistent job storage
- **Redis** for caching and queues
- **Kafka** for message streaming
- **Optional monitoring** with Kafka UI and Redis Commander

## ğŸ“š Documentation

- [Detailed Documentation](docs/README.md)
- [Deployment Guide](docs/DEPLOYMENT.md)
- [API Reference](docs/API.md)

## ğŸ”§ Configuration

### Environment Variables
- `MONGO_URI`: MongoDB connection string
- `REDIS_URL`: Redis connection string
- `KAFKA_BOOTSTRAP_SERVERS`: Kafka servers
- `API_HOST`: API server host
- `API_PORT`: API server port

### Docker Configuration
- [docker-compose.yml](config/docker-compose.yml) - Service orchestration
- [Dockerfile](backend/Dockerfile) - Backend container
- [Frontend Dockerfile](frontend/Dockerfile) - Frontend container

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](docs/LICENSE) file for details.

## ğŸ†˜ Support

For issues and questions:
1. Check the [documentation](docs/README.md)
2. Search existing issues
3. Create a new issue with detailed information

---

**Happy Crawling! ğŸ•·ï¸** 